Quiz
====

Question 1
----------

Suppose (theta)1 is at a local optimum of J((theta)1), such as shown in the figure. What will one step of gradient descent  
(theta)1 := (theta)1 - ((alpha)*((d J((theta)1))/d((theta)1))) do?   
![Graph Problem 1](https://github.com/UtkarshPathrabe/Machine-Learning-Stanford-University-Coursera/blob/master/Week%2001/02.%20Linear%20Regression%20with%20One%20Variable/Lecture06Graph.png)

### Answer

Leave (theta)1 unchanged.

### Explanation

Because the derivative term would be zero.